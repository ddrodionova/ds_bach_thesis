# New Strategies for Improving the Evaluation of Conversational Dialogue Systems

In recent years, due to the development of deep learning techniques, the field of dialogue systems has achieved great results. However, the evaluation is still considered a problematic task, especially in regards to conversational agents. Human assessments are still essential but due to their subjectivity researchers are motivated to work on automated assessment metrics. 

This repository contains code for my Bachelor Thesis at NRU HSE.

The goal of this thesis is to explore different dialogue features to determine what is characterized as good interaction between the bot and the user

1. Calculate or extract dialogue features that can potentially provide information about dialogue rating.
2. Explore the possible correlation between dialogue features and users’ ratings using correlation analysis.
3. Prepare the beta version of the markup tag system, which will serve as the basis for creating a universal web-page interface that will be useful for developing and improving dialogue systems.

I utilize a dataset of conversations between Amazon Alexa users and DREAM Socialbot, a social bot from DeepPavlov, obtained during the Amazon Alexa Prize Challenge 2019.

The analysis confirms the utility of length and sentiment features, as well as the importance of some bot skills and dialogue acts for estimating user’s rating.

## Data

The disclosure of the dataset is not possible due to a Non–disclosure agreement (NDA).
