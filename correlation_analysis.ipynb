{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f0e9376",
   "metadata": {},
   "source": [
    "This is a code example that was run with the help of my science advisor, who provided me with data analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cd9c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "import collections\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c8225",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('alexaprize-export.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0736b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = data.iloc[:352, :]\n",
    "df2 = data.iloc[352:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b955e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FNAMES = []\n",
    "FNAMES.append(df1)\n",
    "FNAMES.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1f9da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs(n=float('inf')):\n",
    "    \n",
    "    for fname in tqdm(FNAMES):\n",
    "        # df = pd.read_json(fname)\n",
    "        df = pd.DataFrame(fname)\n",
    "        \n",
    "    yield df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371944b",
   "metadata": {},
   "source": [
    "## Get length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8b73f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_len():\n",
    "    \n",
    "    length_of_dialogues = []\n",
    "    \n",
    "    for df in dfs():\n",
    "        df['rating'] = df['rating'].fillna(0)\n",
    "        notzero_rated = df.loc[df['rating'] != 0]\n",
    "        rated_utts = notzero_rated['utterances'].tolist()\n",
    "\n",
    "        for i in rated_utts:\n",
    "            length = len(i)\n",
    "            length_of_dialogues.append(length)\n",
    "    \n",
    "        length_data = pd.DataFrame({\n",
    "            'rating': notzero_rated['rating'],\n",
    "            'length_in_turns': length_of_dialogues})\n",
    "    \n",
    "    return length_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length(df):\n",
    "    \n",
    "    lengths_data = {}\n",
    "    utterances = df['utterances'].tolist()\n",
    "    \n",
    "    dia = 0\n",
    "    for dialogue in utterances:\n",
    "        dia += 1\n",
    "        length = len(dialogue)\n",
    "    \n",
    "        lengths_data.update({dia: {'length_in_turns': length}})\n",
    "        lengths_df = pd.DataFrame(lengths_data).T\n",
    "    \n",
    "    return lengths_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e7bb6d",
   "metadata": {},
   "source": [
    "## Get sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65974eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(dfs):\n",
    "    \n",
    "    notzero_rated = dfs[dfs['rating'] != 0]\n",
    "    utterances = notzero_rated['utterances'].tolist()\n",
    "    dialogue_data = {}\n",
    "    dia = 0\n",
    "    for d in utterances:\n",
    "        dia += 1\n",
    "        sentiments = []\n",
    "        for turn in d:\n",
    "            if turn['user']['user_type'] == 'human': \n",
    "                try:\n",
    "                    for key, value in turn['annotations']['combined_classification']['sentiment_classification'].items():\n",
    "                        if key == 'positive':\n",
    "                            sentiments.append(1)\n",
    "                        elif key == 'neutral':\n",
    "                            sentiments.append(0)\n",
    "                        elif key == 'negative':\n",
    "                            sentiments.append(-1)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "        \n",
    "        dialogue_data.update({dia: {'sentiments': sentiments}})\n",
    "        transposed_dialogue_data = pd.DataFrame(dialogue_data).T\n",
    "        \n",
    "    return transposed_dialogue_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44797c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = None\n",
    "\n",
    "def overall_sentiment():\n",
    "    \n",
    "    global a\n",
    "    notnan_dfs = []\n",
    "    for df in dfs():\n",
    "        notnan_df = df.dropna(subset=['rating']).loc[df['rating'] != 0]\n",
    "        if not notnan_df.shape[0]: continue\n",
    "        sentiment_calculated = get_sentiment(notnan_df)\n",
    "        sentiment_calculated.index = notnan_df.index\n",
    "        notnan_df['sentiment'] = sentiment_calculated\n",
    "        a = notnan_df\n",
    "        all_sentiments = [sum(sent) for sent in notnan_df['sentiment']]\n",
    "    \n",
    "        for i in range(len(all_sentiments)):\n",
    "            if all_sentiments[i] > 0:\n",
    "                all_sentiments[i] = 1\n",
    "            elif all_sentiments[i] == 0:\n",
    "                all_sentiments[i] = 0\n",
    "            elif all_sentiments[i] < 0:\n",
    "                all_sentiments[i] = -1\n",
    "    \n",
    "        notnan_df['sentiment'] = all_sentiments\n",
    "        notnan_dfs.append(notnan_df)\n",
    "        \n",
    "    concat_df = pd.concat(notnan_dfs, ignore_index=True)\n",
    "    concat_df.index += 1\n",
    "    \n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71983f65",
   "metadata": {},
   "source": [
    "## Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f659d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skills(df):\n",
    "    \n",
    "    utterances = df['utterances'].tolist()\n",
    "    all_skills = {}\n",
    "    dia = 0\n",
    "    \n",
    "    for dialogue in utterances:\n",
    "        dia += 1\n",
    "        skills = []\n",
    "        for turn in dialogue:\n",
    "            if turn['user']['user_type'] == 'bot':\n",
    "                skills.append(turn['active_skill'])\n",
    "        \n",
    "        freq_skills = collections.Counter(skills)\n",
    "        all_skills.update({dia: {\n",
    "            'skill_name': dict(freq_skills)}})\n",
    "        \n",
    "        transposed_skills = pd.DataFrame(all_skills).T\n",
    "        \n",
    "    return transposed_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d25a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skill_for_dataframes():\n",
    "\n",
    "    notnan_dfs = []\n",
    "    for df in dfs():\n",
    "        notnan_df = df.dropna(subset=['rating']).loc[df['rating'] != 0]\n",
    "        if not notnan_df.shape[0]: continue\n",
    "        all_skills_freq = get_skills(notnan_df)\n",
    "        all_skills_freq.index = notnan_df.index\n",
    "        skills_df = all_skills_freq['skill_name'].apply(pd.Series)\n",
    "        \n",
    "        reset_ix = notnan_df.reset_index(drop=True)\n",
    "        reset_ix.index += 1\n",
    "        skills_df.index = reset_ix.index\n",
    " \n",
    "        full_df = pd.concat([reset_ix, skills_df], axis=1)\n",
    "        notnan_dfs.append(full_df)\n",
    "\n",
    "    concat_df = pd.concat(notnan_dfs, ignore_index=True)\n",
    "    concat_df.index += 1\n",
    "    \n",
    "    return concat_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f336d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKILLS = get_skill_for_dataframes().rename(columns={'': 'no_skill'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351c88ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_dict_spearman = {}\n",
    "\n",
    "for skill in skills_list:\n",
    "    coef, p = spearmanr_corr(SKILLS, skill)\n",
    "    alpha = 0.05\n",
    "    if p > alpha: \n",
    "        skills_dict_spearman.update({skill: {'Spearman': coef,\n",
    "                                    'p_value': p,\n",
    "                                   'correlation': 'no'}})\n",
    "    else:\n",
    "        skills_dict_spearman.update({skill: {'Spearman': coef,\n",
    "                                    'p_value': p,\n",
    "                                   'correlation': 'yes'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47214a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_spearman = pd.DataFrame.from_dict(skills_dict_spearman, \n",
    "                                          orient='index', \n",
    "                                          columns=['Spearman', 'p_value', 'correlation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa996308",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_dict_kendall = {}\n",
    "\n",
    "for skill in skills_list:\n",
    "    coef, p = kendalltau_corr(SKILLS, skill)\n",
    "    alpha = 0.05\n",
    "    if p > alpha: \n",
    "        skills_dict_kendall.update({skill: {'Kendall': coef,\n",
    "                                    'p_value': p,\n",
    "                                   'correlation': 'no'}})\n",
    "    else:\n",
    "        skills_dict_kendall.update({skill: {'Kendall': coef,\n",
    "                                    'p_value': p,\n",
    "                                   'correlation': 'yes'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48eb9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_kendall = pd.DataFrame.from_dict(skills_dict_kendall, \n",
    "                                          orient='index', \n",
    "                                          columns=['Kendall', 'p_value', 'correlation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b867039a",
   "metadata": {},
   "source": [
    "## Dialogue Acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ae4763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_das(df):\n",
    "    \n",
    "    utterances = df['utterances'].tolist()\n",
    "    midas_data = {}\n",
    "    dia = 0\n",
    "    \n",
    "    for dialogue in utterances:\n",
    "        dia += 1\n",
    "        das = []\n",
    "        for turn in dialogue:\n",
    "            if turn.get('user', {'user_type': None})['user_type'] == 'human':\n",
    "                if 'midas_classification' in turn['annotations'].keys():\n",
    "                    if len(turn['annotations']['midas_classification']) != 0:\n",
    "                        das.append(turn['annotations']['midas_classification'][0])\n",
    "                \n",
    "        midas_data.update({dia: {'all_das': das}})\n",
    "    \n",
    "    return midas_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319235d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_das_freq(das_dict):\n",
    "    \n",
    "    avr_das = {}\n",
    "    das_class = {}\n",
    "    dia = 0\n",
    "    \n",
    "    for dialogue, das in das_dict.items():\n",
    "        dia += 1\n",
    "        keys = das['all_das']\n",
    "        if len(keys) != 0:\n",
    "            keys = das['all_das'][0]\n",
    "            for key in keys:\n",
    "                freq = sum(d[key] for d in das['all_das']) / len(das['all_das'])\n",
    "                das_class.update({key: freq})\n",
    "        \n",
    "            avr_das.update({dia: {'all_das': das_class}})\n",
    "            das_class = {}\n",
    "    \n",
    "    list_das = pd.DataFrame(avr_das)\n",
    "    transposed_das = pd.DataFrame(avr_das).T\n",
    "    \n",
    "    return transposed_das"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7645983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_das_for_dataframes():\n",
    "\n",
    "    notnan_dfs = []\n",
    "    for df in dfs():\n",
    "        notnan_df = df.dropna(subset=['rating']).loc[df['rating'] != 0]\n",
    "        if not notnan_df.shape[0]: continue\n",
    "        all_das_freq = get_das_freq(get_das(notnan_df))\n",
    "        all_das_freq.index = notnan_df.index\n",
    "        das_df = all_das_freq['all_das'].apply(pd.Series)\n",
    "        \n",
    "        reset_ix = notnan_df.reset_index(drop=True)\n",
    "        reset_ix.index += 1\n",
    "        das_df.index = reset_ix.index\n",
    " \n",
    "        full_df = pd.concat([reset_ix, das_df], axis=1)\n",
    "        notnan_dfs.append(full_df)\n",
    "\n",
    "    concat_df = pd.concat(notnan_dfs, ignore_index=True)\n",
    "    concat_df.index += 1\n",
    "    \n",
    "    return concat_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f3cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAS = get_das_for_dataframes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739c6153",
   "metadata": {},
   "outputs": [],
   "source": [
    "das_list = ['command', 'comment', 'opinion', 'complaint',\n",
    "            'statement', 'neg_answer', 'pos_answer', 'dev_command', \n",
    "            'appreciation', 'other_answers', 'yes_no_question', \n",
    "            'open_question_factual', 'open_question_opinion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6d9601",
   "metadata": {},
   "outputs": [],
   "source": [
    "das_dict_spearman = {}\n",
    "\n",
    "for das in das_list:\n",
    "    coef, p = spearmanr_corr(DAS, das)\n",
    "    alpha = 0.05\n",
    "    if p > alpha: \n",
    "        das_dict_spearman.update({das: {'Spearman': coef,  'p_value': p, 'correlation': 'no'}})\n",
    "    else:\n",
    "        das_dict_spearman.update({das: {'Spearman': coef,\n",
    "                                    'p_value': p,\n",
    "                                   'correlation': 'yes'}})\n",
    "\n",
    "das_spearman = pd.DataFrame.from_dict(das_dict_spearman, \n",
    "                                          orient='index', \n",
    "                                          columns=['Spearman', 'p_value', 'correlation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be55eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "das_dict_kendall = {}\n",
    "\n",
    "for das in das_list:\n",
    "    coef, p = kendalltau_corr(DAS, das)\n",
    "    alpha = 0.05\n",
    "    if p > alpha: \n",
    "        das_dict_kendall.update({das: {'Kendall': coef,  'p_value': p,  'correlation': 'no'}})\n",
    "    else:\n",
    "        das_dict_kendall.update({das: {'Kendall': coef, 'p_value': p, 'correlation': 'yes'}})\n",
    "\n",
    "das_kendall = pd.DataFrame.from_dict(das_dict_kendall, \n",
    "                                          orient='index', \n",
    "                                          columns=['Kendall', 'p_value', 'correlation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_das = pd.concat([das_pearson, das_spearman, das_kendall], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3f4296",
   "metadata": {},
   "source": [
    "## Sentiment change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06cff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitlist(inputlist, n):\n",
    "    \n",
    "    first_half = inputlist[:n]\n",
    "    sec_half = inputlist[n:]\n",
    "    \n",
    "    return first_half, sec_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b43f599",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = None\n",
    "\n",
    "def sentiment_change():\n",
    "    \n",
    "    global s\n",
    "    notnan_dfs = []\n",
    "    for df in dfs():\n",
    "        notnan_df = df.dropna(subset=['rating']).loc[df['rating'] != 0]\n",
    "        if not notnan_df.shape[0]: continue\n",
    "        sentiment_calculated = get_sentiment(notnan_df)\n",
    "        sentiment_calculated.index = notnan_df.index\n",
    "        notnan_df['sentiment'] = sentiment_calculated\n",
    "        s = notnan_df\n",
    "        \n",
    "        sentiment_changes = []\n",
    "        for sent in notnan_df['sentiment']:\n",
    "            if len(sent) > 3:\n",
    "                first_half, second_half = splitlist(sent, len(sent) // 2)\n",
    "                if sum(first_half) > sum(second_half):\n",
    "                    sentiment_changes.append(-1)\n",
    "                elif sum(first_half) < sum(second_half):\n",
    "                    sentiment_changes.append(1)\n",
    "                else:\n",
    "                    sentiment_changes.append(0)\n",
    "            else:\n",
    "                sentiment_changes.append(-2)\n",
    "        \n",
    "        del notnan_df['sentiment']\n",
    "        notnan_df['sentiment_by_the_end_of_dialogue'] = sentiment_changes\n",
    "        notnan_dfs.append(notnan_df)\n",
    "    \n",
    "    concat_df = pd.concat(notnan_dfs, ignore_index=True)\n",
    "    concat_df.index += 1\n",
    "    \n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764c96b9",
   "metadata": {},
   "source": [
    "## Dialogue ends abruptly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9280af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abrupt_end_dialogue(df):\n",
    "    \n",
    "    utterances = df['utterances'].tolist()\n",
    "    abrupt_dialogue = {}\n",
    "\n",
    "    dialog = 0\n",
    "    for dialogue in utterances:\n",
    "        dialog += 1\n",
    "        last_turn = []\n",
    "        last_skill = []\n",
    "        abruptly_ended = []\n",
    "        for turn in dialogue:\n",
    "            last_turt = []\n",
    "            if turn.get('user', {'user_type': None})['user_type'] == 'bot':\n",
    "                last_turn.append(turn['text'])\n",
    "                last_skill.append(turn['active_skill'])\n",
    "\n",
    "        if last_turn[-1][-1] == '?':\n",
    "            abruptly_ended.append(1)\n",
    "        else:\n",
    "            abruptly_ended.append(0)\n",
    "\n",
    "        abrupt_dialogue.update({dialog: {'abrupt_end': abruptly_ended[0],\n",
    "                                        'last_skill': last_skill[-1]}})\n",
    "        transposed_dialogue_data = pd.DataFrame(abrupt_dialogue).T\n",
    "        \n",
    "    return transposed_dialogue_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842096a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abrupt_end_for_dataframes():\n",
    "\n",
    "    notnan_dfs = []\n",
    "    for df in dfs():\n",
    "        notnan_df = df.dropna(subset=['rating']).loc[df['rating'] != 0]\n",
    "        if not notnan_df.shape[0]: continue\n",
    "        all_ends = abrupt_end_dialogue(notnan_df)\n",
    "        # all_ends.index = notnan_df.index\n",
    "        \n",
    "        reset_ix = notnan_df.reset_index(drop=True)\n",
    "        reset_ix.index += 1\n",
    "        all_ends.index = reset_ix.index\n",
    " \n",
    "        full_df = pd.concat([reset_ix, all_ends], axis=1)\n",
    "        notnan_dfs.append(full_df)\n",
    "\n",
    "    concat_df = pd.concat(notnan_dfs, ignore_index=True)\n",
    "    concat_df.index += 1\n",
    "    \n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17caa11",
   "metadata": {},
   "source": [
    "## User's average answers are too short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7953a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_answers(df):\n",
    "    \n",
    "    utterances = df['utterances'].tolist()\n",
    "    short_data = {}\n",
    "    dia = 0\n",
    "    \n",
    "    for dialogue in utterances:\n",
    "        dia += 1\n",
    "        das = []\n",
    "        sentence_length = []\n",
    "        \n",
    "        for turn in dialogue:\n",
    "            if turn.get('user', {'user_type': None})['user_type'] == 'human':\n",
    "                words = turn['text'].split()\n",
    "                sentence_length.append(words)\n",
    "            \n",
    "        turn_lengths = []\n",
    "        average_user_reply = []\n",
    "        for sentence in sentence_length:\n",
    "            turn_lengths.append(len(sentence))\n",
    "        average_user_reply.append(mean(turn_lengths))\n",
    "        \n",
    "        short_reply = []\n",
    "        for reply in average_user_reply:\n",
    "            if reply > 1 and reply < 2:\n",
    "                short_reply.append(1)\n",
    "            else:\n",
    "                short_reply.append(0)\n",
    "        \n",
    "      \n",
    "        short_data.update({dia: {'average_length_too_short': short_reply[0]}})\n",
    "    \n",
    "    short_reply_list = pd.DataFrame(short_data).T\n",
    "    \n",
    "    return short_reply_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ababf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_short_answers_for_dataframes():\n",
    "\n",
    "    notnan_dfs = []\n",
    "    for df in dfs():\n",
    "        notnan_df = df.dropna(subset=['rating']).loc[df['rating'] != 0]\n",
    "        if not notnan_df.shape[0]: continue\n",
    "        all_ends = short_answers(notnan_df)\n",
    "        # all_ends.index = notnan_df.index\n",
    "        \n",
    "        reset_ix = notnan_df.reset_index(drop=True)\n",
    "        reset_ix.index += 1\n",
    "        all_ends.index = reset_ix.index\n",
    " \n",
    "        full_df = pd.concat([reset_ix, all_ends], axis=1)\n",
    "        notnan_dfs.append(full_df)\n",
    "\n",
    "    concat_df = pd.concat(notnan_dfs, ignore_index=True)\n",
    "    concat_df.index += 1\n",
    "    \n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eadc027",
   "metadata": {},
   "source": [
    "## Skills change too frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277247b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_change_skill(df):\n",
    "    \n",
    "    utterances = df['utterances'].tolist()\n",
    "    change_skill = {}\n",
    "\n",
    "    dialog = 0\n",
    "    for dialogue in utterances:\n",
    "        dialog += 1\n",
    "        all_skills = []\n",
    "        for turn in dialogue:\n",
    "            if turn.get('user', {'user_type': None})['user_type'] == 'bot':\n",
    "                all_skills.append(turn['active_skill'])\n",
    "\n",
    "        idx = 0\n",
    "        result = []\n",
    "        while idx < len(all_skills)-1:\n",
    "            if len(all_skills) != 1:\n",
    "                if all_skills[idx] == all_skills[idx+1]:\n",
    "                    result.append(0)\n",
    "                    # print(all_skills[idx], all_skills[idx+1], 'are the same')\n",
    "                elif all_skills[idx] != all_skills[idx+1]:\n",
    "                    result.append(1)\n",
    "                    # print(all_skills[idx], all_skills[idx+1], 'are not the same')\n",
    "                idx += 1\n",
    "        \n",
    "        change_skill.update({dialog: {'is_next_skill_the_same': dict(collections.Counter(result))}})\n",
    "        \n",
    "        skill_changes_too_freq = {}\n",
    "        freq_skill = [] \n",
    "        for k, value in change_skill.items():\n",
    "            keys = value['is_next_skill_the_same']\n",
    "            if len(keys) != 0:\n",
    "                f = max(keys, key=keys.get)\n",
    "                freq_skill.append(f)\n",
    "            else:\n",
    "                freq_skill.append(2)\n",
    "\n",
    "    skill_changes_too_freq.update({'skill_changes_too_freq': freq_skill})\n",
    "    freq_skills_df = pd.DataFrame(skill_changes_too_freq)\n",
    "    freq_skills_df.index += 1 \n",
    "\n",
    "    return freq_skills_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66297689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skill_change_for_dataframes():\n",
    "\n",
    "    notnan_dfs = []\n",
    "    for df in dfs():\n",
    "        notnan_df = df.dropna(subset=['rating']).loc[df['rating'] != 0]\n",
    "        if not notnan_df.shape[0]: continue\n",
    "        all_das = freq_change_skill(notnan_df)\n",
    "        all_das.index = notnan_df.index\n",
    "        \n",
    "        full_df = pd.concat([notnan_df, all_das], axis=1)\n",
    "        notnan_dfs.append(full_df)\n",
    "        \n",
    "    concat_df = pd.concat(notnan_dfs, ignore_index=True)\n",
    "    concat_df.index += 1\n",
    "    \n",
    "    return concat_df.fillna(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75331cdc",
   "metadata": {},
   "source": [
    "## Calculate all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48670860",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = None\n",
    "\n",
    "def find_bad_features():\n",
    "    \n",
    "    global a\n",
    "    notnan_dfs = []\n",
    "    for df in dfs():\n",
    "        notnan_df = df.dropna(subset=['rating']).loc[df['rating'] != 0]\n",
    "        if not notnan_df.shape[0]: continue\n",
    "            \n",
    "        # length\n",
    "        length_obtained = get_length(notnan_df)\n",
    "        length_obtained.index = notnan_df.index\n",
    "        notnan_df['length_in_turns'] = length_obtained\n",
    "            \n",
    "        # sentiment\n",
    "        sentiment_calculated = get_sentiment(notnan_df)\n",
    "        sentiment_calculated.index = notnan_df.index\n",
    "        notnan_df['sentiment'] = sentiment_calculated\n",
    "        a = notnan_df\n",
    "        all_sentiments = [sum(sent) for sent in notnan_df['sentiment']]\n",
    "    \n",
    "        for i in range(len(all_sentiments)):\n",
    "            if all_sentiments[i] > 0:\n",
    "                all_sentiments[i] = 1\n",
    "            elif all_sentiments[i] == 0:\n",
    "                all_sentiments[i] = 0\n",
    "            elif all_sentiments[i] < 0:\n",
    "                all_sentiments[i] = -1\n",
    "    \n",
    "        notnan_df['overall_sentiment'] = all_sentiments\n",
    "        \n",
    "        # sentiment change\n",
    "        sentiment_changes = []\n",
    "        for sent in notnan_df['sentiment']:\n",
    "            if len(sent) > 3:\n",
    "                first_half, second_half = splitlist(sent, len(sent) // 2)\n",
    "                if sum(first_half) > sum(second_half):\n",
    "                    sentiment_changes.append(-1)\n",
    "                elif sum(first_half) < sum(second_half):\n",
    "                    sentiment_changes.append(1)\n",
    "                else:\n",
    "                    sentiment_changes.append(0)\n",
    "            else:\n",
    "                sentiment_changes.append(2)\n",
    "        \n",
    "        del notnan_df['sentiment']\n",
    "        notnan_df['sentiment_by_the_end_of_dialogue'] = sentiment_changes\n",
    "\n",
    "        reset_ix = notnan_df.reset_index(drop=True)\n",
    "        reset_ix.index += 1\n",
    "        \n",
    "        # dialogue ends too abruptly\n",
    "        all_ends = abrupt_end_dialogue(reset_ix)\n",
    "        \n",
    "        # user's answers are mostly very short\n",
    "        short_reply = short_answers(reset_ix)\n",
    " \n",
    "        # skills change frequently\n",
    "        all_skills = freq_change_skill(reset_ix)\n",
    "        \n",
    "        new_df = pd.concat([reset_ix, all_ends, \n",
    "                            short_reply, all_skills], axis=1) \n",
    "        notnan_dfs.append(new_df)\n",
    "    \n",
    "    concat_df = pd.concat(notnan_dfs, ignore_index=True)\n",
    "    concat_df.index += 1\n",
    "    \n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e6ce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIALOGUES = find_bad_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
