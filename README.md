# New Strategies for Improving the Evaluation of Conversational Dialogue Systems

In recent years, due to the development of deep learning techniques, the field of dialogue systems has achieved great results. However, the evaluation is still considered a problematic task, especially in regards to conversational agents. Human assessments are still essential but due to their subjectivity researchers are motivated to work on automated assessment metrics. 

The purpose of this thesis is to propose new ways to improve the automatic evaluation metrics of conversational systems. To do this, I will explore different dialogue features to determine what is characterized as good interaction between the bot and the user using the correlation analysis. I utilize a dataset of conversations between Amazon Alexa users and DREAM Socialbot, a social bot from DeepPavlov, obtained during the Amazon Alexa Prize Challenge 2019.

The analysis confirms the utility of length and sentiment features, as well as the importance of some bot skills and dialogue acts for estimating user’s rating.

## Data

The disclosure of the dataset is not possible due to a Non–disclosure agreement (NDA).
